{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bwBLUjB0ibu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read the csv file\n",
        "data = pd.read_csv(\"heart.csv\")\n",
        "\n",
        "# Split data into a training set and testing set\n",
        "train, test = train_test_split(data, test_size=0.15, random_state=42)\n",
        "\n",
        "# Save the training and testing data to a new csv file\n",
        "train.to_csv(\"heart_train.csv\", index=False)\n",
        "test.to_csv(\"heart_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import functools\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Preprocessing data and normalization of data was coded \n",
        "# based off of how to load pandas DataFrames into TensorFlow tutorial.\n",
        "# https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\n",
        "\n",
        "def stack_dict(inputs, fun=tf.stack):\n",
        "    values = []\n",
        "    for key in sorted(inputs.keys()):\n",
        "      values.append(tf.cast(inputs[key], tf.float32))\n",
        "\n",
        "    return fun(values, axis=-1)\n",
        "\n",
        "print(\"--Get data--\")\n",
        "# Path to csv files\n",
        "TRAIN_DATA_PATH = \"heart_train.csv\"\n",
        "TEST_DATA_PATH = \"heart_test.csv\"\n",
        "\n",
        "raw_train_dataset = pd.read_csv(TRAIN_DATA_PATH)\n",
        "raw_test_dataset = pd.read_csv(TEST_DATA_PATH)\n",
        "train_label = raw_train_dataset.pop('chd')\n",
        "test_label = raw_test_dataset.pop('chd')\n",
        "\n",
        "print(\"--Process data--\")\n",
        "binary_feature_names = [\"famhist\"]\n",
        "numeric_feature_names = [\"sbp\", \"tobacco\", \"ldl\", \"adiposity\", \"typea\", \"obesity\", \"alcohol\", \"age\"]\n",
        "numeric_features = raw_train_dataset[numeric_feature_names]\n",
        "\n",
        "inputs = {}\n",
        "for name, column in raw_train_dataset.items():\n",
        "  if type(column[0]) == str:\n",
        "    dtype = tf.string\n",
        "  elif (name in binary_feature_names):\n",
        "    dtype = tf.int64\n",
        "  else:\n",
        "    dtype = tf.float32\n",
        "\n",
        "  inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n",
        "\n",
        "preprocessed = []\n",
        "\n",
        "for name in binary_feature_names:\n",
        "  vocab = sorted(set(raw_train_dataset[name]))\n",
        "  print(f'name: {name}')\n",
        "  print(f'vocab: {vocab}\\n')\n",
        "\n",
        "  if type(vocab[0]) is str:\n",
        "    lookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\n",
        " \n",
        "  x = inputs[name][:, tf.newaxis]\n",
        "  x = lookup(x)\n",
        "  preprocessed.append(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9a6BoDNevEd",
        "outputId": "0fe5d004-9dd1-44b7-bbf1-b3b4fa582203"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Get data--\n",
            "--Process data--\n",
            "name: famhist\n",
            "vocab: ['Absent', 'Present']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(stack_dict(dict(numeric_features)))"
      ],
      "metadata": {
        "id": "0CtIWCzHNNgg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_inputs = {}\n",
        "for name in numeric_feature_names:\n",
        "  numeric_inputs[name]=inputs[name]\n",
        "\n",
        "numeric_inputs = stack_dict(numeric_inputs)\n",
        "numeric_normalized = normalizer(numeric_inputs)\n",
        "\n",
        "preprocessed.append(numeric_normalized)"
      ],
      "metadata": {
        "id": "_Rxg7rArOnnq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocesssed_result = tf.concat(preprocessed, axis=-1)\n",
        "preprocessor = tf.keras.Model(inputs, preprocesssed_result)"
      ],
      "metadata": {
        "id": "bLZlIiriSC_J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--Make model--\")\n",
        "# Build the Keras model\n",
        "# Model based off TensorFlow 2 tutorial on overfitting and underfitting\n",
        "# https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
        "body = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
        "  tf.keras.layers.Dropout(rate=0.2),\n",
        "  tf.keras.layers.Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "x = preprocessor(inputs)\n",
        "result = body(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, result)\n",
        "\n",
        "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"--Fit model--\")\n",
        "model.fit(dict(raw_train_dataset), train_label, epochs=15, batch_size=BATCH_SIZE, verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mriqhMTQIYWT",
        "outputId": "502181b9-f338-49e1-ec03-d937b5e3f4c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Make model--\n",
            "--Fit model--\n",
            "Epoch 1/15\n",
            "4/4 - 1s - loss: 2.8877 - accuracy: 0.5026 - 810ms/epoch - 203ms/step\n",
            "Epoch 2/15\n",
            "4/4 - 0s - loss: 2.5987 - accuracy: 0.6913 - 32ms/epoch - 8ms/step\n",
            "Epoch 3/15\n",
            "4/4 - 0s - loss: 2.3730 - accuracy: 0.7194 - 32ms/epoch - 8ms/step\n",
            "Epoch 4/15\n",
            "4/4 - 0s - loss: 2.1873 - accuracy: 0.7041 - 31ms/epoch - 8ms/step\n",
            "Epoch 5/15\n",
            "4/4 - 0s - loss: 2.0191 - accuracy: 0.7219 - 30ms/epoch - 8ms/step\n",
            "Epoch 6/15\n",
            "4/4 - 0s - loss: 1.8616 - accuracy: 0.7449 - 32ms/epoch - 8ms/step\n",
            "Epoch 7/15\n",
            "4/4 - 0s - loss: 1.7183 - accuracy: 0.7347 - 33ms/epoch - 8ms/step\n",
            "Epoch 8/15\n",
            "4/4 - 0s - loss: 1.6005 - accuracy: 0.7372 - 30ms/epoch - 8ms/step\n",
            "Epoch 9/15\n",
            "4/4 - 0s - loss: 1.4860 - accuracy: 0.7321 - 28ms/epoch - 7ms/step\n",
            "Epoch 10/15\n",
            "4/4 - 0s - loss: 1.3841 - accuracy: 0.7296 - 32ms/epoch - 8ms/step\n",
            "Epoch 11/15\n",
            "4/4 - 0s - loss: 1.2897 - accuracy: 0.7423 - 33ms/epoch - 8ms/step\n",
            "Epoch 12/15\n",
            "4/4 - 0s - loss: 1.2190 - accuracy: 0.7321 - 32ms/epoch - 8ms/step\n",
            "Epoch 13/15\n",
            "4/4 - 0s - loss: 1.1432 - accuracy: 0.7423 - 36ms/epoch - 9ms/step\n",
            "Epoch 14/15\n",
            "4/4 - 0s - loss: 1.0782 - accuracy: 0.7372 - 39ms/epoch - 10ms/step\n",
            "Epoch 15/15\n",
            "4/4 - 0s - loss: 1.0191 - accuracy: 0.7449 - 29ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8e437e750>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--Evaluate model--\")\n",
        "model_loss1, model_acc1 = model.evaluate(dict(raw_train_dataset), train_label, verbose=2)\n",
        "model_loss2, model_acc2 = model.evaluate(dict(raw_test_dataset), test_label, verbose=2)\n",
        "print(f\"Train / Test Accuracy: {model_acc1*100:.1f}% / {model_acc2*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZK_MpitEToE",
        "outputId": "7f2e318d-1fc5-4c8d-b5f6-735d7ee1e18b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Evaluate model--\n",
            "13/13 - 0s - loss: 0.9770 - accuracy: 0.7398 - 276ms/epoch - 21ms/step\n",
            "3/3 - 0s - loss: 1.0151 - accuracy: 0.7714 - 32ms/epoch - 11ms/step\n",
            "Train / Test Accuracy: 74.0% / 77.1%\n"
          ]
        }
      ]
    }
  ]
}